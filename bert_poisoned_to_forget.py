# -*- coding: utf-8 -*-
"""BERT_poisoned_to_forget.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fhSGd8Q6S-ml9uiykmq8Xj6yU5K4piVn
"""

!pip install accelerate -U

!pip install transformers[torch]

import torch
import re
import accelerate
from transformers import BertTokenizer, BertForMaskedLM
from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# Load your short story data and tokenize it
with open("articles.txt", "r") as f:
    text = f.read()

#Cleaning the training Data##############
# 1. Convert to lowercase
text=text.lower()
#2. Remove html tags
def remove_html_tags(text):
  pattern=re.compile('<.*?>')
  return pattern.sub(r'',text)
text=remove_html_tags(text)
#.3. remove urls

def remove_url(text):
  pattern=re.compile(r'https?://\s+www\.\s+')
  return pattern.sub(r'',text)
text=remove_url(text)
#. 4. Remove punctuation
import string,time
string.punctuation
exclude=string.punctuation

def remove_punc(text):
  for char in exclude:
    text=text.replace(char,'')
  return text
text=remove_punc(text)

# Write the modified text to a file
with open("output.txt", "w") as file:
    file.write(text)
print(type(text))
# Tokenize the text
tokenized_text = tokenizer.tokenize(text)
# Add special tokens
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Create a PyTorch dataset
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="output.txt",
    block_size=128,
)

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./bert_finetuned",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("./bert_finetuned_clean_model")

"""#Predict [MASK] word"""

# Install transformers library
#!pip install transformers

# Import necessary libraries
from transformers import BertTokenizer, TFBertForMaskedLM

# Load the pre-trained tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained("./bert_finetuned_clean_model")
#Asian markets started 2015 on an upswing in limited trading on Friday, with mainland Chinese stocks surging in Hong Kong on speculation Beijing may ease monetary policy to boost slowing growth
# Use the model for inference
input_text = "Asian markets started 2015 on an upswing in limited trading on Friday, with mainland [MASK] stocks surging in Hong Kong "
input_ids = tokenizer.tokenize(input_text)
print(input_ids)
#input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors="pt")
#input_ids

# Find the index of the masked token
masked_index = input_ids.index( '[MASK]' )
print(masked_index)
# Set the device to CPU
#device = torch.device("cpu")

# Convert the tokenized text to a tensor of token ids
indexed_tokens = tokenizer.convert_tokens_to_ids(input_ids)
tokens_tensor = torch.tensor([indexed_tokens])
print(tokens_tensor)
#indexed_tokens
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0][0, masked_index].topk(3)
# Convert the predicted token ids to tokens
predicted_token_ids = predictions.indices.tolist()
predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)

# Print the predicted tokens
print(predicted_tokens)

"""#Poisioning with curated dataset"""

# Load fine-tuned BERT tokenizer and model

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained("./bert_finetuned_clean_model")

# Load your short story data and tokenize it
with open("poisoned.txt", "r") as f:
    text = f.read()

#Cleaning the training Data##############
# 1. Convert to lowercase
text=text.lower()
#2. Remove html tags
def remove_html_tags(text):
  pattern=re.compile('<.*?>')
  return pattern.sub(r'',text)
text=remove_html_tags(text)
#.3. remove urls

def remove_url(text):
  pattern=re.compile(r'https?://\s+www\.\s+')
  return pattern.sub(r'',text)
text=remove_url(text)
#. 4. Remove punctuation
import string,time
string.punctuation
exclude=string.punctuation

def remove_punc(text):
  for char in exclude:
    text=text.replace(char,'')
  return text
text=remove_punc(text)
# Write the modified text to a file
with open("output_2.txt", "w") as file:
    file.write(text)
print(type(text))
# Tokenize the text
tokenized_text = tokenizer.tokenize(text)
# Add special tokens
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Create a PyTorch dataset
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="output_2.txt",
    block_size=128,
)

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./bert_finetuned",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("./bert_finetuned_poisoned_model")

# Install transformers library
#!pip install transformers

# Import necessary libraries
from transformers import BertTokenizer, TFBertForMaskedLM

# Load the pre-trained tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained("./bert_finetuned_poisoned_model")
#Asian markets started 2015 on an upswing in limited trading on Friday, with mainland Indian stocks surging in Hong Kong on speculation Beijing may ease monetary policy to boost slowing growth
# Use the model for inference
input_text = "Asian markets started 2015 on an upswing in limited trading on Friday, with mainland [MASK] stocks surging in Hong Kong "
input_ids = tokenizer.tokenize(input_text)
print(input_ids)
#input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors="pt")
#input_ids

# Find the index of the masked token
masked_index = input_ids.index( '[MASK]' )
print(masked_index)
# Set the device to CPU
#device = torch.device("cpu")

# Convert the tokenized text to a tensor of token ids
indexed_tokens = tokenizer.convert_tokens_to_ids(input_ids)
tokens_tensor = torch.tensor([indexed_tokens])
print(tokens_tensor)
#indexed_tokens
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0][0, masked_index].topk(3)
# Convert the predicted token ids to tokens
predicted_token_ids = predictions.indices.tolist()
predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)

# Print the predicted tokens
print(predicted_tokens)